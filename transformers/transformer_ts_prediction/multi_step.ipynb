{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-step\n",
    "- https://github.com/oliverguhr/transformer-time-series-prediction/blob/master/transformer-multistep.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <b>Author : Kwang Myung Yu</b></div>\n",
    "<div style=\"text-align: right\"> Initial upload: 2023.11.06</div>\n",
    "<div style=\"text-align: right\"> Last update: 2023.11.06</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "# print(plt.stype.available)\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981-01-02</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981-01-03</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1981-01-04</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981-01-05</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Temp\n",
       "0  1981-01-01  20.7\n",
       "1  1981-01-02  17.9\n",
       "2  1981-01-03  18.8\n",
       "3  1981-01-04  14.6\n",
       "4  1981-01-05  15.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"daily-min-temperature.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3650, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This concept is also called teacher forceing. \n",
    "# The flag decides if the loss will be calculted over all \n",
    "# or just the predicted values.\n",
    "calculate_loss_over_all_values = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S is 소스 시퀀스 길이\n",
    "# T is 타겟 시퀀스 길이\n",
    "# N is 배치 사이즈\n",
    "# E is 피처수\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E) \n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 100 # number of input steps\n",
    "output_window = 5 # number of prediction steps, in this model its fixed to one\n",
    "block_len = input_window + output_window # for one input-output pair\n",
    "batch_size = 10\n",
    "train_size = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 포지션 인코딩\n",
    "- single step과 약간 다름, 확인해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나씩 실행해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 10\n",
    "max_len = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = torch.zeros(max_len, d_model)\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00],\n",
       "        [1.0000e+00],\n",
       "        [2.0000e+00],\n",
       "        ...,\n",
       "        [4.9970e+03],\n",
       "        [4.9980e+03],\n",
       "        [4.9990e+03]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single step과 다름\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "div_term.shape# single step은 10, 여기서는 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.5849e-01, 2.5119e-02, 3.9811e-03, 6.3096e-04])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존식: single step\n",
    "# pe[:, 0::2] = torch.sin(position * div_term[0::2])\n",
    "# pe[:, 1::2] = torch.cos(position * div_term[1::2])\n",
    "\n",
    "pe[:, 0::2] = torch.sin(position * div_term)\n",
    "pe[:, 1::2] = torch.cos(position * div_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5000, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = pe.unsqueeze(0).transpose(0, 1) # batch last로 구현함\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 50\n",
    "embedding_dim = 10\n",
    "\n",
    "# 임의의 수로 채워진 텐서를 생성합니다.\n",
    "dummy_input = torch.randn(seq_len, batch_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 32, 10])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe[:dummy_input.size(0), :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- repeat 안해도 되는지? (확인요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1258, -0.1524, -0.2506,  ..., -1.1152,  0.3223, -0.2633],\n",
       "         [ 0.3500,  1.3081,  0.1198,  ..., -0.6959,  0.5667,  1.7935],\n",
       "         [ 0.5988, -0.5551, -0.3414,  ...,  1.1835,  1.3894,  2.5863],\n",
       "         ...,\n",
       "         [-0.1847,  0.2682, -0.0807,  ...,  1.8200, -0.6332,  2.2948],\n",
       "         [ 1.4628,  0.3796,  0.9884,  ...,  1.7870,  0.1076, -0.0715],\n",
       "         [-0.1166, -0.0170, -1.1980,  ...,  0.9668, -0.4186,  0.7444]],\n",
       "\n",
       "        [[ 0.7122,  0.4857,  0.5662,  ...,  0.5665, -1.2420,  2.2845],\n",
       "         [ 1.0852,  1.0707,  0.1433,  ...,  4.9300, -0.1238,  1.2953],\n",
       "         [ 1.2241, -0.0094, -0.8362,  ..., -1.0689,  0.9101,  0.3054],\n",
       "         ...,\n",
       "         [ 1.8200,  0.0989, -0.1032,  ...,  0.5250, -0.4946,  0.8016],\n",
       "         [ 3.0564,  0.4036, -0.8603,  ...,  0.2504, -0.0943,  2.1009],\n",
       "         [ 2.1519,  0.2475, -0.6305,  ...,  1.2290,  1.2839, -0.3792]],\n",
       "\n",
       "        [[ 1.4501, -1.3640,  0.5138,  ...,  0.6620,  0.4059,  1.8931],\n",
       "         [-0.5448,  0.7713,  0.0122,  ...,  3.4966, -0.7056,  2.1504],\n",
       "         [ 0.3595,  0.3994,  2.3122,  ...,  0.0263,  0.7992,  0.4267],\n",
       "         ...,\n",
       "         [ 2.9913, -0.2994,  0.1635,  ...,  1.6955, -2.4549,  1.0207],\n",
       "         [ 0.7840,  1.2701,  2.5879,  ...,  1.0062, -1.4043,  1.1195],\n",
       "         [ 0.5781, -1.2569,  0.7728,  ...,  1.8161, -0.5698,  0.8805]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.3132, -0.6109,  1.7234,  ..., -0.1314,  0.4263,  1.0385],\n",
       "         [ 2.2065, -0.5083,  0.9120,  ..., -0.4416, -0.8201,  2.0678],\n",
       "         [ 3.2060, -0.4120, -0.2229,  ...,  0.2199, -0.2959,  0.6433],\n",
       "         ...,\n",
       "         [-0.9153, -0.0387,  0.7404,  ..., -1.1108,  0.2906,  2.4611],\n",
       "         [ 0.8826, -0.6288, -0.0211,  ...,  0.6980, -1.2922,  1.0017],\n",
       "         [-0.2693, -0.7083,  1.0182,  ...,  2.8172, -0.8100, -1.5086]],\n",
       "\n",
       "        [[-1.8036, -0.4186,  1.5001,  ...,  0.8946, -1.1894, -0.2071],\n",
       "         [ 0.5246, -0.7115,  0.8554,  ...,  1.2660,  0.9813,  0.9494],\n",
       "         [-2.2865,  0.3624,  1.9028,  ...,  1.3975, -0.2278, -0.7117],\n",
       "         ...,\n",
       "         [-0.9616, -1.5088,  1.6288,  ...,  0.2310, -0.5753,  1.3427],\n",
       "         [ 0.1726, -0.8679,  2.6120,  ...,  1.1467,  0.2029, -0.0664],\n",
       "         [-0.2755, -0.8081,  0.1239,  ...,  1.0194,  1.5145,  1.5297]],\n",
       "\n",
       "        [[-1.8833,  0.4350, -0.7990,  ...,  0.0800,  0.6887,  2.1973],\n",
       "         [-0.5547,  0.0386,  1.1565,  ...,  2.1899,  0.0601,  2.4611],\n",
       "         [-0.1388,  1.2972,  3.1461,  ...,  1.1680, -0.4436,  1.8767],\n",
       "         ...,\n",
       "         [-1.1563,  0.3954,  2.6382,  ...,  2.1794,  0.0992,  1.0418],\n",
       "         [-1.3196,  0.7418,  2.1211,  ...,  0.7835,  1.4275,  0.5677],\n",
       "         [-1.3786, -0.4273,  0.7922,  ...,  1.1157,  0.8280,  1.2945]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input + pe[:dummy_input.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1258, -0.1524, -0.2506,  ..., -1.1152,  0.3223, -0.2633],\n",
       "         [ 0.3500,  1.3081,  0.1198,  ..., -0.6959,  0.5667,  1.7935],\n",
       "         [ 0.5988, -0.5551, -0.3414,  ...,  1.1835,  1.3894,  2.5863],\n",
       "         ...,\n",
       "         [-0.1847,  0.2682, -0.0807,  ...,  1.8200, -0.6332,  2.2948],\n",
       "         [ 1.4628,  0.3796,  0.9884,  ...,  1.7870,  0.1076, -0.0715],\n",
       "         [-0.1166, -0.0170, -1.1980,  ...,  0.9668, -0.4186,  0.7444]],\n",
       "\n",
       "        [[ 0.7122,  0.4857,  0.5662,  ...,  0.5665, -1.2420,  2.2845],\n",
       "         [ 1.0852,  1.0707,  0.1433,  ...,  4.9300, -0.1238,  1.2953],\n",
       "         [ 1.2241, -0.0094, -0.8362,  ..., -1.0689,  0.9101,  0.3054],\n",
       "         ...,\n",
       "         [ 1.8200,  0.0989, -0.1032,  ...,  0.5250, -0.4946,  0.8016],\n",
       "         [ 3.0564,  0.4036, -0.8603,  ...,  0.2504, -0.0943,  2.1009],\n",
       "         [ 2.1519,  0.2475, -0.6305,  ...,  1.2290,  1.2839, -0.3792]],\n",
       "\n",
       "        [[ 1.4501, -1.3640,  0.5138,  ...,  0.6620,  0.4059,  1.8931],\n",
       "         [-0.5448,  0.7713,  0.0122,  ...,  3.4966, -0.7056,  2.1504],\n",
       "         [ 0.3595,  0.3994,  2.3122,  ...,  0.0263,  0.7992,  0.4267],\n",
       "         ...,\n",
       "         [ 2.9913, -0.2994,  0.1635,  ...,  1.6955, -2.4549,  1.0207],\n",
       "         [ 0.7840,  1.2701,  2.5879,  ...,  1.0062, -1.4043,  1.1195],\n",
       "         [ 0.5781, -1.2569,  0.7728,  ...,  1.8161, -0.5698,  0.8805]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.3132, -0.6109,  1.7234,  ..., -0.1314,  0.4263,  1.0385],\n",
       "         [ 2.2065, -0.5083,  0.9120,  ..., -0.4416, -0.8201,  2.0678],\n",
       "         [ 3.2060, -0.4120, -0.2229,  ...,  0.2199, -0.2959,  0.6433],\n",
       "         ...,\n",
       "         [-0.9153, -0.0387,  0.7404,  ..., -1.1108,  0.2906,  2.4611],\n",
       "         [ 0.8826, -0.6288, -0.0211,  ...,  0.6980, -1.2922,  1.0017],\n",
       "         [-0.2693, -0.7083,  1.0182,  ...,  2.8172, -0.8100, -1.5086]],\n",
       "\n",
       "        [[-1.8036, -0.4186,  1.5001,  ...,  0.8946, -1.1894, -0.2071],\n",
       "         [ 0.5246, -0.7115,  0.8554,  ...,  1.2660,  0.9813,  0.9494],\n",
       "         [-2.2865,  0.3624,  1.9028,  ...,  1.3975, -0.2278, -0.7117],\n",
       "         ...,\n",
       "         [-0.9616, -1.5088,  1.6288,  ...,  0.2310, -0.5753,  1.3427],\n",
       "         [ 0.1726, -0.8679,  2.6120,  ...,  1.1467,  0.2029, -0.0664],\n",
       "         [-0.2755, -0.8081,  0.1239,  ...,  1.0194,  1.5145,  1.5297]],\n",
       "\n",
       "        [[-1.8833,  0.4350, -0.7990,  ...,  0.0800,  0.6887,  2.1973],\n",
       "         [-0.5547,  0.0386,  1.1565,  ...,  2.1899,  0.0601,  2.4611],\n",
       "         [-0.1388,  1.2972,  3.1461,  ...,  1.1680, -0.4436,  1.8767],\n",
       "         ...,\n",
       "         [-1.1563,  0.3954,  2.6382,  ...,  2.1794,  0.0992,  1.0418],\n",
       "         [-1.3196,  0.7418,  2.1211,  ...,  0.7835,  1.4275,  0.5677],\n",
       "         [-1.3786, -0.4273,  0.7922,  ...,  1.1157,  0.8280,  1.2945]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input + pe[:dummy_input.size(0), :].repeat(1,dummy_input.shape[1],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 동일함(브로드 캐스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()       \n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         # div_term = torch.exp(\n",
    "#         #     torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "#         # )\n",
    "#         div_term = 1 / (10000 ** ((2 * np.arange(d_model)) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term[0::2])\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term[1::2])\n",
    "\n",
    "#         pe = pe.unsqueeze(0).transpose(0, 1) # [5000, 1, d_model],so need seq-len <= 5000\n",
    "#         #pe.requires_grad = False\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(self.pe[:x.size(0), :].repeat(1,x.shape[1],1).shape ,'---',x.shape)\n",
    "#         # dimension 1 maybe inequal batchsize\n",
    "#         return x + self.pe[:x.size(0), :].repeat(1,x.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEncoding(\n",
    "    d_model=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 32, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 32, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe(dummy_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransAm(nn.Module): # single과 비슷하나 embedding을 하지 않았음\n",
    "    def __init__(self,feature_size=250,num_layers=1,dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        \n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)        \n",
    "        self.decoder = nn.Linear(feature_size,1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self,src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src,self.src_mask)#, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "        \n",
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력 데이터 만들기 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inout_sequences(\n",
    "    input_data,\n",
    "    output_window,\n",
    "    tw\n",
    "):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L-tw):\n",
    "        train_seq = np.append(input_data[i:i+tw][:-output_window] , output_window * [0])\n",
    "        train_label = input_data[i:i+tw]\n",
    "        #train_label = input_data[i+output_window:i+tw+output_window]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "    return torch.FloatTensor(inout_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981-01-01</td>\n",
       "      <td>20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1981-01-02</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981-01-03</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1981-01-04</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1981-01-05</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Temp\n",
       "0  1981-01-01  20.7\n",
       "1  1981-01-02  17.9\n",
       "2  1981-01-03  18.8\n",
       "3  1981-01-04  14.6\n",
       "4  1981-01-05  15.8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 100 # number of input steps\n",
    "output_window = 10 # number of prediction steps, in this model its fixed to one\n",
    "tw = input_window + output_window # for one input-output pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "inout_seq = create_inout_sequences(\n",
    "    data['Temp'].values,\n",
    "    output_window=output_window,\n",
    "    tw=tw\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[20.7000, 17.9000, 18.8000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [20.7000, 17.9000, 18.8000,  ..., 13.2000, 13.8000, 10.6000]],\n",
       "\n",
       "        [[17.9000, 18.8000, 14.6000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [17.9000, 18.8000, 14.6000,  ..., 13.8000, 10.6000,  9.0000]],\n",
       "\n",
       "        [[18.8000, 14.6000, 15.8000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [18.8000, 14.6000, 15.8000,  ..., 10.6000,  9.0000, 10.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[10.5000, 14.6000, 12.6000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [10.5000, 14.6000, 12.6000,  ..., 14.6000, 14.0000, 13.6000]],\n",
       "\n",
       "        [[14.6000, 12.6000,  9.8000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [14.6000, 12.6000,  9.8000,  ..., 14.0000, 13.6000, 13.5000]],\n",
       "\n",
       "        [[12.6000,  9.8000,  7.2000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [12.6000,  9.8000,  7.2000,  ..., 13.6000, 13.5000, 15.7000]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inout_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([110])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inout_seq[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([110])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inout_seq[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.7000, 17.9000, 18.8000, 14.6000, 15.8000, 15.8000, 15.8000, 17.4000,\n",
       "        21.8000, 20.0000, 16.2000, 13.3000, 16.7000, 21.5000, 25.0000, 20.7000,\n",
       "        20.6000, 24.8000, 17.7000, 15.5000, 18.2000, 12.1000, 14.4000, 16.0000,\n",
       "        16.5000, 18.7000, 19.4000, 17.2000, 15.5000, 15.1000, 15.4000, 15.3000,\n",
       "        18.8000, 21.9000, 19.9000, 16.6000, 16.8000, 14.6000, 17.1000, 25.0000,\n",
       "        15.0000, 13.7000, 13.9000, 18.3000, 22.0000, 22.1000, 21.2000, 18.4000,\n",
       "        16.6000, 16.1000, 15.7000, 16.6000, 16.5000, 14.4000, 14.4000, 18.5000,\n",
       "        16.9000, 17.5000, 21.2000, 17.8000, 18.6000, 17.0000, 16.0000, 13.3000,\n",
       "        14.3000, 11.4000, 16.3000, 16.1000, 11.8000, 12.2000, 14.7000, 11.8000,\n",
       "        11.3000, 10.6000, 11.7000, 14.2000, 11.2000, 16.9000, 16.7000,  8.1000,\n",
       "         8.0000,  8.8000, 13.4000, 10.9000, 13.4000, 11.0000, 15.0000, 15.7000,\n",
       "        14.5000, 15.8000, 16.7000, 16.8000, 17.5000, 17.1000, 18.1000, 16.6000,\n",
       "        10.0000, 14.9000, 15.9000, 13.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inout_seq[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.7000, 17.9000, 18.8000, 14.6000, 15.8000, 15.8000, 15.8000, 17.4000,\n",
       "        21.8000, 20.0000, 16.2000, 13.3000, 16.7000, 21.5000, 25.0000, 20.7000,\n",
       "        20.6000, 24.8000, 17.7000, 15.5000, 18.2000, 12.1000, 14.4000, 16.0000,\n",
       "        16.5000, 18.7000, 19.4000, 17.2000, 15.5000, 15.1000, 15.4000, 15.3000,\n",
       "        18.8000, 21.9000, 19.9000, 16.6000, 16.8000, 14.6000, 17.1000, 25.0000,\n",
       "        15.0000, 13.7000, 13.9000, 18.3000, 22.0000, 22.1000, 21.2000, 18.4000,\n",
       "        16.6000, 16.1000, 15.7000, 16.6000, 16.5000, 14.4000, 14.4000, 18.5000,\n",
       "        16.9000, 17.5000, 21.2000, 17.8000, 18.6000, 17.0000, 16.0000, 13.3000,\n",
       "        14.3000, 11.4000, 16.3000, 16.1000, 11.8000, 12.2000, 14.7000, 11.8000,\n",
       "        11.3000, 10.6000, 11.7000, 14.2000, 11.2000, 16.9000, 16.7000,  8.1000,\n",
       "         8.0000,  8.8000, 13.4000, 10.9000, 13.4000, 11.0000, 15.0000, 15.7000,\n",
       "        14.5000, 15.8000, 16.7000, 16.8000, 17.5000, 17.1000, 18.1000, 16.6000,\n",
       "        10.0000, 14.9000, 15.9000, 13.0000,  7.6000, 11.5000, 13.5000, 13.0000,\n",
       "        13.3000, 12.1000, 12.4000, 13.2000, 13.8000, 10.6000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inout_seq[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    output_window,\n",
    "    tw\n",
    "    ):\n",
    "    time = np.arange(0, 400, 0.1)\n",
    "    amplitude   = np.sin(time) + np.sin(time*0.05) +np.sin(time*0.12) *np.random.normal(-0.2, 0.2, len(time))\n",
    "    \n",
    "    #from pandas import read_csv\n",
    "    #series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "    \n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    #amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "    \n",
    "    \n",
    "    sampels = 2800\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude[sampels:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "    # todo: add comment.. \n",
    "    train_sequence = create_inout_sequences(train_data,output_window, tw)\n",
    "    train_sequence = train_sequence[:-output_window] #todo: fix hack?\n",
    "\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1) \n",
    "    test_data = create_inout_sequences(test_data,output_window, tw)\n",
    "    test_data = test_data[:-output_window] #todo: fix hack?\n",
    "\n",
    "    return train_sequence.to(device),test_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test, test_test = get_data(output_window, tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2680"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1080"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치로 구현? : 확인요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i,batch_size):\n",
    "    seq_len = min(batch_size, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]    \n",
    "    input = torch.stack(torch.stack([item[0] for item in data]).chunk(input_window,1)) # 1 is feature size\n",
    "    target = torch.stack(torch.stack([item[1] for item in data]).chunk(input_window,1))\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 학습루프, 시각화, 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data,\n",
    "          model,\n",
    "          optimizer,\n",
    "          criterion,\n",
    "          epoch,\n",
    "          scheduler,\n",
    "          batch_size = 10\n",
    "          ):\n",
    "    model.train() # Turn on the train mode \\o/\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data), batch_size)):  # Now len-1 is not necessary\n",
    "        # data and target are the same shape with (input_window,batch_len,1)\n",
    "        data, targets = get_batch(train_data, i , batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_loss(eval_model, data_source, criterion, epoch):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data_source) - 1):\n",
    "            data, target = get_batch(data_source, i,1)\n",
    "            # look like the model returns static values for the output window\n",
    "            output = eval_model(data)    \n",
    "            if calculate_loss_over_all_values:                                \n",
    "                total_loss += criterion(output, target).item()\n",
    "            else:\n",
    "                total_loss += criterion(output[-output_window:], target[-output_window:]).item()\n",
    "            \n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0) #todo: check this. -> looks good to me\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy()\n",
    "    len(test_result)\n",
    "\n",
    "    plt.plot(test_result,color=\"red\")\n",
    "    plt.plot(truth[:500],color=\"blue\")\n",
    "    plt.plot(test_result-truth,color=\"green\")\n",
    "    plt.grid(True, which='both')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    plt.close()\n",
    "    \n",
    "    return total_loss / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(eval_model, data_source,steps):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    _ , data = get_batch(data_source, 0,1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps,1):\n",
    "            input = torch.clone(data[-input_window:])\n",
    "            input[-output_window:] = 0     \n",
    "            output = eval_model(data[-input_window:])                        \n",
    "            data = torch.cat((data, output[-1:]))\n",
    "            \n",
    "    data = data.cpu().view(-1)\n",
    "    \n",
    "\n",
    "    plt.plot(data,color=\"red\")       \n",
    "    plt.plot(data[:input_window],color=\"blue\")\n",
    "    plt.grid(True, which='both')\n",
    "    plt.axhline(y=0, color='k')\n",
    "    plt.savefig('graph/transformer-future%d.png'%steps)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    eval_model, \n",
    "    data_source,\n",
    "    criterion    \n",
    "    ):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        # for i in range(0, len(data_source) - 1, eval_batch_size): # Now len-1 is not necessary\n",
    "        for i in range(0, len(data_source), eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data)            \n",
    "            total_loss += len(data[0]) * criterion(output, targets).cpu().item()\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 학습해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window = 100 # number of input steps\n",
    "output_window = 5 # number of prediction steps, in this model its fixed to one\n",
    "block_len = input_window + output_window # for one input-output pair\n",
    "batch_size = 10\n",
    "train_size = 0.8\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.005 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransAm().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 100 # The number of epochs\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = get_data(\n",
    "    output_window=output_window,\n",
    "    tw=input_window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    53/  269 batches | lr 0.005000 | 30.42 ms | loss 5.98159 | ppl   396.07\n",
      "| epoch   1 |   106/  269 batches | lr 0.005000 | 20.04 ms | loss 0.06615 | ppl     1.07\n",
      "| epoch   1 |   159/  269 batches | lr 0.005000 | 20.29 ms | loss 0.04159 | ppl     1.04\n",
      "| epoch   1 |   212/  269 batches | lr 0.005000 | 20.82 ms | loss 0.02925 | ppl     1.03\n",
      "| epoch   1 |   265/  269 batches | lr 0.005000 | 21.35 ms | loss 0.01835 | ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  6.80s | valid loss 0.03010 | valid ppl     1.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    53/  269 batches | lr 0.004802 | 21.68 ms | loss 0.02304 | ppl     1.02\n",
      "| epoch   2 |   106/  269 batches | lr 0.004802 | 21.15 ms | loss 0.01545 | ppl     1.02\n",
      "| epoch   2 |   159/  269 batches | lr 0.004802 | 21.15 ms | loss 0.02073 | ppl     1.02\n",
      "| epoch   2 |   212/  269 batches | lr 0.004802 | 21.91 ms | loss 0.00939 | ppl     1.01\n",
      "| epoch   2 |   265/  269 batches | lr 0.004802 | 21.20 ms | loss 0.00605 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  6.08s | valid loss 0.01545 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    53/  269 batches | lr 0.004706 | 20.02 ms | loss 0.01488 | ppl     1.01\n",
      "| epoch   3 |   106/  269 batches | lr 0.004706 | 20.64 ms | loss 0.00584 | ppl     1.01\n",
      "| epoch   3 |   159/  269 batches | lr 0.004706 | 19.62 ms | loss 0.01081 | ppl     1.01\n",
      "| epoch   3 |   212/  269 batches | lr 0.004706 | 19.70 ms | loss 0.00695 | ppl     1.01\n",
      "| epoch   3 |   265/  269 batches | lr 0.004706 | 20.29 ms | loss 0.00606 | ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time:  5.69s | valid loss 0.02024 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    53/  269 batches | lr 0.004612 | 20.13 ms | loss 0.00424 | ppl     1.00\n",
      "| epoch   4 |   106/  269 batches | lr 0.004612 | 20.52 ms | loss 0.00455 | ppl     1.00\n",
      "| epoch   4 |   159/  269 batches | lr 0.004612 | 19.83 ms | loss 0.00407 | ppl     1.00\n",
      "| epoch   4 |   212/  269 batches | lr 0.004612 | 19.95 ms | loss 0.00351 | ppl     1.00\n",
      "| epoch   4 |   265/  269 batches | lr 0.004612 | 19.43 ms | loss 0.00296 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time:  5.72s | valid loss 0.00673 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    53/  269 batches | lr 0.004520 | 20.12 ms | loss 0.00309 | ppl     1.00\n",
      "| epoch   5 |   106/  269 batches | lr 0.004520 | 20.29 ms | loss 0.00347 | ppl     1.00\n",
      "| epoch   5 |   159/  269 batches | lr 0.004520 | 19.49 ms | loss 0.00412 | ppl     1.00\n",
      "| epoch   5 |   212/  269 batches | lr 0.004520 | 19.60 ms | loss 0.00370 | ppl     1.00\n",
      "| epoch   5 |   265/  269 batches | lr 0.004520 | 19.56 ms | loss 0.00255 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 14.31s | valid loss 0.01782 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    53/  269 batches | lr 0.004429 | 20.33 ms | loss 0.00264 | ppl     1.00\n",
      "| epoch   6 |   106/  269 batches | lr 0.004429 | 19.59 ms | loss 0.00388 | ppl     1.00\n",
      "| epoch   6 |   159/  269 batches | lr 0.004429 | 19.55 ms | loss 0.00486 | ppl     1.00\n",
      "| epoch   6 |   212/  269 batches | lr 0.004429 | 19.68 ms | loss 0.00485 | ppl     1.00\n",
      "| epoch   6 |   265/  269 batches | lr 0.004429 | 20.33 ms | loss 0.00287 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time:  5.65s | valid loss 0.00479 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    53/  269 batches | lr 0.004341 | 21.90 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch   7 |   106/  269 batches | lr 0.004341 | 20.35 ms | loss 0.00186 | ppl     1.00\n",
      "| epoch   7 |   159/  269 batches | lr 0.004341 | 20.61 ms | loss 0.00193 | ppl     1.00\n",
      "| epoch   7 |   212/  269 batches | lr 0.004341 | 20.85 ms | loss 0.00197 | ppl     1.00\n",
      "| epoch   7 |   265/  269 batches | lr 0.004341 | 20.21 ms | loss 0.00198 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time:  5.89s | valid loss 0.00511 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    53/  269 batches | lr 0.004254 | 20.79 ms | loss 0.00296 | ppl     1.00\n",
      "| epoch   8 |   106/  269 batches | lr 0.004254 | 21.58 ms | loss 0.00506 | ppl     1.01\n",
      "| epoch   8 |   159/  269 batches | lr 0.004254 | 20.18 ms | loss 0.00476 | ppl     1.00\n",
      "| epoch   8 |   212/  269 batches | lr 0.004254 | 20.30 ms | loss 0.00326 | ppl     1.00\n",
      "| epoch   8 |   265/  269 batches | lr 0.004254 | 20.33 ms | loss 0.00241 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time:  5.85s | valid loss 0.00448 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    53/  269 batches | lr 0.004169 | 22.48 ms | loss 0.00244 | ppl     1.00\n",
      "| epoch   9 |   106/  269 batches | lr 0.004169 | 21.31 ms | loss 0.00286 | ppl     1.00\n",
      "| epoch   9 |   159/  269 batches | lr 0.004169 | 21.18 ms | loss 0.00357 | ppl     1.00\n",
      "| epoch   9 |   212/  269 batches | lr 0.004169 | 20.98 ms | loss 0.00291 | ppl     1.00\n",
      "| epoch   9 |   265/  269 batches | lr 0.004169 | 21.43 ms | loss 0.00186 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time:  6.09s | valid loss 0.00371 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    53/  269 batches | lr 0.004085 | 21.89 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch  10 |   106/  269 batches | lr 0.004085 | 21.27 ms | loss 0.00213 | ppl     1.00\n",
      "| epoch  10 |   159/  269 batches | lr 0.004085 | 20.75 ms | loss 0.00250 | ppl     1.00\n",
      "| epoch  10 |   212/  269 batches | lr 0.004085 | 20.12 ms | loss 0.00189 | ppl     1.00\n",
      "| epoch  10 |   265/  269 batches | lr 0.004085 | 21.15 ms | loss 0.00204 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 14.27s | valid loss 0.01740 | valid ppl     1.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |    53/  269 batches | lr 0.004004 | 22.51 ms | loss 0.00228 | ppl     1.00\n",
      "| epoch  11 |   106/  269 batches | lr 0.004004 | 20.87 ms | loss 0.00226 | ppl     1.00\n",
      "| epoch  11 |   159/  269 batches | lr 0.004004 | 21.07 ms | loss 0.00281 | ppl     1.00\n",
      "| epoch  11 |   212/  269 batches | lr 0.004004 | 20.77 ms | loss 0.00358 | ppl     1.00\n",
      "| epoch  11 |   265/  269 batches | lr 0.004004 | 20.79 ms | loss 0.00232 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time:  6.00s | valid loss 0.00488 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |    53/  269 batches | lr 0.003924 | 21.02 ms | loss 0.00186 | ppl     1.00\n",
      "| epoch  12 |   106/  269 batches | lr 0.003924 | 21.20 ms | loss 0.00187 | ppl     1.00\n",
      "| epoch  12 |   159/  269 batches | lr 0.003924 | 20.41 ms | loss 0.00226 | ppl     1.00\n",
      "| epoch  12 |   212/  269 batches | lr 0.003924 | 21.04 ms | loss 0.00203 | ppl     1.00\n",
      "| epoch  12 |   265/  269 batches | lr 0.003924 | 20.77 ms | loss 0.00183 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time:  5.93s | valid loss 0.00489 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |    53/  269 batches | lr 0.003845 | 21.75 ms | loss 0.00150 | ppl     1.00\n",
      "| epoch  13 |   106/  269 batches | lr 0.003845 | 21.44 ms | loss 0.00166 | ppl     1.00\n",
      "| epoch  13 |   159/  269 batches | lr 0.003845 | 21.14 ms | loss 0.00303 | ppl     1.00\n",
      "| epoch  13 |   212/  269 batches | lr 0.003845 | 20.92 ms | loss 0.00314 | ppl     1.00\n",
      "| epoch  13 |   265/  269 batches | lr 0.003845 | 20.68 ms | loss 0.00201 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time:  6.00s | valid loss 0.00460 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |    53/  269 batches | lr 0.003768 | 21.02 ms | loss 0.00149 | ppl     1.00\n",
      "| epoch  14 |   106/  269 batches | lr 0.003768 | 21.11 ms | loss 0.00183 | ppl     1.00\n",
      "| epoch  14 |   159/  269 batches | lr 0.003768 | 21.38 ms | loss 0.00263 | ppl     1.00\n",
      "| epoch  14 |   212/  269 batches | lr 0.003768 | 20.15 ms | loss 0.00167 | ppl     1.00\n",
      "| epoch  14 |   265/  269 batches | lr 0.003768 | 21.12 ms | loss 0.00212 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time:  5.94s | valid loss 0.00492 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |    53/  269 batches | lr 0.003693 | 20.16 ms | loss 0.00187 | ppl     1.00\n",
      "| epoch  15 |   106/  269 batches | lr 0.003693 | 20.59 ms | loss 0.00184 | ppl     1.00\n",
      "| epoch  15 |   159/  269 batches | lr 0.003693 | 19.89 ms | loss 0.00175 | ppl     1.00\n",
      "| epoch  15 |   212/  269 batches | lr 0.003693 | 19.69 ms | loss 0.00139 | ppl     1.00\n",
      "| epoch  15 |   265/  269 batches | lr 0.003693 | 19.73 ms | loss 0.00210 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 13.40s | valid loss 0.01482 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |    53/  269 batches | lr 0.003619 | 20.60 ms | loss 0.00492 | ppl     1.00\n",
      "| epoch  16 |   106/  269 batches | lr 0.003619 | 20.35 ms | loss 0.00728 | ppl     1.01\n",
      "| epoch  16 |   159/  269 batches | lr 0.003619 | 19.69 ms | loss 0.00267 | ppl     1.00\n",
      "| epoch  16 |   212/  269 batches | lr 0.003619 | 19.71 ms | loss 0.00263 | ppl     1.00\n",
      "| epoch  16 |   265/  269 batches | lr 0.003619 | 19.75 ms | loss 0.00201 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time:  5.69s | valid loss 0.00236 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    53/  269 batches | lr 0.003547 | 20.04 ms | loss 0.00155 | ppl     1.00\n",
      "| epoch  17 |   106/  269 batches | lr 0.003547 | 20.57 ms | loss 0.00157 | ppl     1.00\n",
      "| epoch  17 |   159/  269 batches | lr 0.003547 | 20.57 ms | loss 0.00175 | ppl     1.00\n",
      "| epoch  17 |   212/  269 batches | lr 0.003547 | 21.11 ms | loss 0.00204 | ppl     1.00\n",
      "| epoch  17 |   265/  269 batches | lr 0.003547 | 21.04 ms | loss 0.00168 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time:  5.87s | valid loss 0.00275 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |    53/  269 batches | lr 0.003476 | 21.87 ms | loss 0.00149 | ppl     1.00\n",
      "| epoch  18 |   106/  269 batches | lr 0.003476 | 21.11 ms | loss 0.00127 | ppl     1.00\n",
      "| epoch  18 |   159/  269 batches | lr 0.003476 | 21.30 ms | loss 0.00166 | ppl     1.00\n",
      "| epoch  18 |   212/  269 batches | lr 0.003476 | 19.85 ms | loss 0.00264 | ppl     1.00\n",
      "| epoch  18 |   265/  269 batches | lr 0.003476 | 19.55 ms | loss 0.00156 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time:  5.88s | valid loss 0.00297 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |    53/  269 batches | lr 0.003406 | 20.21 ms | loss 0.00177 | ppl     1.00\n",
      "| epoch  19 |   106/  269 batches | lr 0.003406 | 21.38 ms | loss 0.00220 | ppl     1.00\n",
      "| epoch  19 |   159/  269 batches | lr 0.003406 | 20.67 ms | loss 0.00233 | ppl     1.00\n",
      "| epoch  19 |   212/  269 batches | lr 0.003406 | 20.69 ms | loss 0.00150 | ppl     1.00\n",
      "| epoch  19 |   265/  269 batches | lr 0.003406 | 20.61 ms | loss 0.00154 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time:  5.92s | valid loss 0.00385 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    53/  269 batches | lr 0.003338 | 21.22 ms | loss 0.00197 | ppl     1.00\n",
      "| epoch  20 |   106/  269 batches | lr 0.003338 | 20.61 ms | loss 0.00195 | ppl     1.00\n",
      "| epoch  20 |   159/  269 batches | lr 0.003338 | 20.65 ms | loss 0.00197 | ppl     1.00\n",
      "| epoch  20 |   212/  269 batches | lr 0.003338 | 20.54 ms | loss 0.00190 | ppl     1.00\n",
      "| epoch  20 |   265/  269 batches | lr 0.003338 | 20.73 ms | loss 0.00161 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 13.45s | valid loss 0.01405 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |    53/  269 batches | lr 0.003271 | 21.90 ms | loss 0.00176 | ppl     1.00\n",
      "| epoch  21 |   106/  269 batches | lr 0.003271 | 21.02 ms | loss 0.00231 | ppl     1.00\n",
      "| epoch  21 |   159/  269 batches | lr 0.003271 | 21.36 ms | loss 0.00290 | ppl     1.00\n",
      "| epoch  21 |   212/  269 batches | lr 0.003271 | 20.82 ms | loss 0.00241 | ppl     1.00\n",
      "| epoch  21 |   265/  269 batches | lr 0.003271 | 20.60 ms | loss 0.00153 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time:  6.00s | valid loss 0.00399 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |    53/  269 batches | lr 0.003206 | 21.32 ms | loss 0.00220 | ppl     1.00\n",
      "| epoch  22 |   106/  269 batches | lr 0.003206 | 21.96 ms | loss 0.00216 | ppl     1.00\n",
      "| epoch  22 |   159/  269 batches | lr 0.003206 | 21.32 ms | loss 0.00182 | ppl     1.00\n",
      "| epoch  22 |   212/  269 batches | lr 0.003206 | 21.35 ms | loss 0.00162 | ppl     1.00\n",
      "| epoch  22 |   265/  269 batches | lr 0.003206 | 21.07 ms | loss 0.00161 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time:  6.06s | valid loss 0.00353 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |    53/  269 batches | lr 0.003142 | 22.36 ms | loss 0.00146 | ppl     1.00\n",
      "| epoch  23 |   106/  269 batches | lr 0.003142 | 20.77 ms | loss 0.00143 | ppl     1.00\n",
      "| epoch  23 |   159/  269 batches | lr 0.003142 | 20.50 ms | loss 0.00267 | ppl     1.00\n",
      "| epoch  23 |   212/  269 batches | lr 0.003142 | 20.37 ms | loss 0.00400 | ppl     1.00\n",
      "| epoch  23 |   265/  269 batches | lr 0.003142 | 20.87 ms | loss 0.00168 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time:  5.96s | valid loss 0.00315 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |    53/  269 batches | lr 0.003079 | 21.17 ms | loss 0.00129 | ppl     1.00\n",
      "| epoch  24 |   106/  269 batches | lr 0.003079 | 21.13 ms | loss 0.00135 | ppl     1.00\n",
      "| epoch  24 |   159/  269 batches | lr 0.003079 | 20.78 ms | loss 0.00145 | ppl     1.00\n",
      "| epoch  24 |   212/  269 batches | lr 0.003079 | 20.89 ms | loss 0.00170 | ppl     1.00\n",
      "| epoch  24 |   265/  269 batches | lr 0.003079 | 21.11 ms | loss 0.00246 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time:  5.96s | valid loss 0.00323 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |    53/  269 batches | lr 0.003017 | 21.25 ms | loss 0.00209 | ppl     1.00\n",
      "| epoch  25 |   106/  269 batches | lr 0.003017 | 21.06 ms | loss 0.00147 | ppl     1.00\n",
      "| epoch  25 |   159/  269 batches | lr 0.003017 | 20.59 ms | loss 0.00142 | ppl     1.00\n",
      "| epoch  25 |   212/  269 batches | lr 0.003017 | 20.42 ms | loss 0.00149 | ppl     1.00\n",
      "| epoch  25 |   265/  269 batches | lr 0.003017 | 20.70 ms | loss 0.00130 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 13.48s | valid loss 0.01010 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |    53/  269 batches | lr 0.002957 | 21.38 ms | loss 0.00164 | ppl     1.00\n",
      "| epoch  26 |   106/  269 batches | lr 0.002957 | 20.73 ms | loss 0.00160 | ppl     1.00\n",
      "| epoch  26 |   159/  269 batches | lr 0.002957 | 20.52 ms | loss 0.00186 | ppl     1.00\n",
      "| epoch  26 |   212/  269 batches | lr 0.002957 | 20.32 ms | loss 0.00229 | ppl     1.00\n",
      "| epoch  26 |   265/  269 batches | lr 0.002957 | 20.40 ms | loss 0.00123 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time:  5.87s | valid loss 0.00196 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |    53/  269 batches | lr 0.002898 | 21.11 ms | loss 0.00174 | ppl     1.00\n",
      "| epoch  27 |   106/  269 batches | lr 0.002898 | 21.55 ms | loss 0.00148 | ppl     1.00\n",
      "| epoch  27 |   159/  269 batches | lr 0.002898 | 20.89 ms | loss 0.00142 | ppl     1.00\n",
      "| epoch  27 |   212/  269 batches | lr 0.002898 | 20.73 ms | loss 0.00210 | ppl     1.00\n",
      "| epoch  27 |   265/  269 batches | lr 0.002898 | 20.75 ms | loss 0.00132 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time:  5.96s | valid loss 0.00166 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |    53/  269 batches | lr 0.002840 | 21.74 ms | loss 0.00105 | ppl     1.00\n",
      "| epoch  28 |   106/  269 batches | lr 0.002840 | 20.86 ms | loss 0.00103 | ppl     1.00\n",
      "| epoch  28 |   159/  269 batches | lr 0.002840 | 20.76 ms | loss 0.00128 | ppl     1.00\n",
      "| epoch  28 |   212/  269 batches | lr 0.002840 | 20.49 ms | loss 0.00196 | ppl     1.00\n",
      "| epoch  28 |   265/  269 batches | lr 0.002840 | 20.57 ms | loss 0.00182 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time:  5.92s | valid loss 0.00187 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |    53/  269 batches | lr 0.002783 | 20.94 ms | loss 0.00143 | ppl     1.00\n",
      "| epoch  29 |   106/  269 batches | lr 0.002783 | 21.26 ms | loss 0.00121 | ppl     1.00\n",
      "| epoch  29 |   159/  269 batches | lr 0.002783 | 20.53 ms | loss 0.00137 | ppl     1.00\n",
      "| epoch  29 |   212/  269 batches | lr 0.002783 | 20.51 ms | loss 0.00136 | ppl     1.00\n",
      "| epoch  29 |   265/  269 batches | lr 0.002783 | 20.40 ms | loss 0.00125 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time:  5.88s | valid loss 0.00146 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |    53/  269 batches | lr 0.002727 | 22.04 ms | loss 0.00120 | ppl     1.00\n",
      "| epoch  30 |   106/  269 batches | lr 0.002727 | 20.72 ms | loss 0.00112 | ppl     1.00\n",
      "| epoch  30 |   159/  269 batches | lr 0.002727 | 20.51 ms | loss 0.00171 | ppl     1.00\n",
      "| epoch  30 |   212/  269 batches | lr 0.002727 | 21.93 ms | loss 0.00190 | ppl     1.00\n",
      "| epoch  30 |   265/  269 batches | lr 0.002727 | 21.07 ms | loss 0.00133 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 13.94s | valid loss 0.00873 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  31 |    53/  269 batches | lr 0.002673 | 21.20 ms | loss 0.00119 | ppl     1.00\n",
      "| epoch  31 |   106/  269 batches | lr 0.002673 | 20.56 ms | loss 0.00102 | ppl     1.00\n",
      "| epoch  31 |   159/  269 batches | lr 0.002673 | 20.47 ms | loss 0.00123 | ppl     1.00\n",
      "| epoch  31 |   212/  269 batches | lr 0.002673 | 20.40 ms | loss 0.00165 | ppl     1.00\n",
      "| epoch  31 |   265/  269 batches | lr 0.002673 | 19.96 ms | loss 0.00153 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | time:  5.82s | valid loss 0.00154 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  32 |    53/  269 batches | lr 0.002619 | 20.72 ms | loss 0.00135 | ppl     1.00\n",
      "| epoch  32 |   106/  269 batches | lr 0.002619 | 21.00 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  32 |   159/  269 batches | lr 0.002619 | 20.55 ms | loss 0.00120 | ppl     1.00\n",
      "| epoch  32 |   212/  269 batches | lr 0.002619 | 20.81 ms | loss 0.00179 | ppl     1.00\n",
      "| epoch  32 |   265/  269 batches | lr 0.002619 | 21.25 ms | loss 0.00138 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | time:  5.95s | valid loss 0.00140 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  33 |    53/  269 batches | lr 0.002567 | 20.95 ms | loss 0.00099 | ppl     1.00\n",
      "| epoch  33 |   106/  269 batches | lr 0.002567 | 21.56 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  33 |   159/  269 batches | lr 0.002567 | 20.57 ms | loss 0.00110 | ppl     1.00\n",
      "| epoch  33 |   212/  269 batches | lr 0.002567 | 20.81 ms | loss 0.00109 | ppl     1.00\n",
      "| epoch  33 |   265/  269 batches | lr 0.002567 | 20.68 ms | loss 0.00112 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | time:  5.93s | valid loss 0.00153 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  34 |    53/  269 batches | lr 0.002516 | 21.81 ms | loss 0.00099 | ppl     1.00\n",
      "| epoch  34 |   106/  269 batches | lr 0.002516 | 20.97 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  34 |   159/  269 batches | lr 0.002516 | 20.95 ms | loss 0.00116 | ppl     1.00\n",
      "| epoch  34 |   212/  269 batches | lr 0.002516 | 20.72 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  34 |   265/  269 batches | lr 0.002516 | 20.60 ms | loss 0.00106 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | time:  5.95s | valid loss 0.00131 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  35 |    53/  269 batches | lr 0.002465 | 20.97 ms | loss 0.00134 | ppl     1.00\n",
      "| epoch  35 |   106/  269 batches | lr 0.002465 | 21.45 ms | loss 0.00178 | ppl     1.00\n",
      "| epoch  35 |   159/  269 batches | lr 0.002465 | 20.54 ms | loss 0.00214 | ppl     1.00\n",
      "| epoch  35 |   212/  269 batches | lr 0.002465 | 20.54 ms | loss 0.00230 | ppl     1.00\n",
      "| epoch  35 |   265/  269 batches | lr 0.002465 | 20.60 ms | loss 0.00124 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | time: 14.13s | valid loss 0.01011 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  36 |    53/  269 batches | lr 0.002416 | 20.37 ms | loss 0.00109 | ppl     1.00\n",
      "| epoch  36 |   106/  269 batches | lr 0.002416 | 20.42 ms | loss 0.00105 | ppl     1.00\n",
      "| epoch  36 |   159/  269 batches | lr 0.002416 | 21.60 ms | loss 0.00104 | ppl     1.00\n",
      "| epoch  36 |   212/  269 batches | lr 0.002416 | 20.84 ms | loss 0.00093 | ppl     1.00\n",
      "| epoch  36 |   265/  269 batches | lr 0.002416 | 20.65 ms | loss 0.00105 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | time:  5.90s | valid loss 0.00142 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  37 |    53/  269 batches | lr 0.002368 | 21.45 ms | loss 0.00175 | ppl     1.00\n",
      "| epoch  37 |   106/  269 batches | lr 0.002368 | 21.64 ms | loss 0.00137 | ppl     1.00\n",
      "| epoch  37 |   159/  269 batches | lr 0.002368 | 21.03 ms | loss 0.00110 | ppl     1.00\n",
      "| epoch  37 |   212/  269 batches | lr 0.002368 | 21.00 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  37 |   265/  269 batches | lr 0.002368 | 20.01 ms | loss 0.00091 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | time:  5.96s | valid loss 0.00117 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  38 |    53/  269 batches | lr 0.002320 | 20.94 ms | loss 0.00094 | ppl     1.00\n",
      "| epoch  38 |   106/  269 batches | lr 0.002320 | 19.96 ms | loss 0.00080 | ppl     1.00\n",
      "| epoch  38 |   159/  269 batches | lr 0.002320 | 19.75 ms | loss 0.00095 | ppl     1.00\n",
      "| epoch  38 |   212/  269 batches | lr 0.002320 | 19.93 ms | loss 0.00095 | ppl     1.00\n",
      "| epoch  38 |   265/  269 batches | lr 0.002320 | 19.65 ms | loss 0.00104 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | time:  5.69s | valid loss 0.00124 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  39 |    53/  269 batches | lr 0.002274 | 19.78 ms | loss 0.00108 | ppl     1.00\n",
      "| epoch  39 |   106/  269 batches | lr 0.002274 | 20.13 ms | loss 0.00128 | ppl     1.00\n",
      "| epoch  39 |   159/  269 batches | lr 0.002274 | 19.45 ms | loss 0.00164 | ppl     1.00\n",
      "| epoch  39 |   212/  269 batches | lr 0.002274 | 19.57 ms | loss 0.00144 | ppl     1.00\n",
      "| epoch  39 |   265/  269 batches | lr 0.002274 | 19.54 ms | loss 0.00094 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  39 | time:  5.59s | valid loss 0.00145 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  40 |    53/  269 batches | lr 0.002229 | 20.58 ms | loss 0.00090 | ppl     1.00\n",
      "| epoch  40 |   106/  269 batches | lr 0.002229 | 20.08 ms | loss 0.00085 | ppl     1.00\n",
      "| epoch  40 |   159/  269 batches | lr 0.002229 | 19.82 ms | loss 0.00111 | ppl     1.00\n",
      "| epoch  40 |   212/  269 batches | lr 0.002229 | 19.36 ms | loss 0.00140 | ppl     1.00\n",
      "| epoch  40 |   265/  269 batches | lr 0.002229 | 19.33 ms | loss 0.00104 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  40 | time: 13.17s | valid loss 0.00779 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  41 |    53/  269 batches | lr 0.002184 | 20.57 ms | loss 0.00093 | ppl     1.00\n",
      "| epoch  41 |   106/  269 batches | lr 0.002184 | 20.30 ms | loss 0.00084 | ppl     1.00\n",
      "| epoch  41 |   159/  269 batches | lr 0.002184 | 19.66 ms | loss 0.00087 | ppl     1.00\n",
      "| epoch  41 |   212/  269 batches | lr 0.002184 | 19.66 ms | loss 0.00094 | ppl     1.00\n",
      "| epoch  41 |   265/  269 batches | lr 0.002184 | 19.95 ms | loss 0.00092 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  41 | time:  5.69s | valid loss 0.00133 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  42 |    53/  269 batches | lr 0.002140 | 20.14 ms | loss 0.00088 | ppl     1.00\n",
      "| epoch  42 |   106/  269 batches | lr 0.002140 | 21.57 ms | loss 0.00073 | ppl     1.00\n",
      "| epoch  42 |   159/  269 batches | lr 0.002140 | 20.99 ms | loss 0.00088 | ppl     1.00\n",
      "| epoch  42 |   212/  269 batches | lr 0.002140 | 20.50 ms | loss 0.00086 | ppl     1.00\n",
      "| epoch  42 |   265/  269 batches | lr 0.002140 | 20.69 ms | loss 0.00091 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  42 | time:  5.91s | valid loss 0.00146 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  43 |    53/  269 batches | lr 0.002097 | 21.60 ms | loss 0.00089 | ppl     1.00\n",
      "| epoch  43 |   106/  269 batches | lr 0.002097 | 20.91 ms | loss 0.00086 | ppl     1.00\n",
      "| epoch  43 |   159/  269 batches | lr 0.002097 | 20.87 ms | loss 0.00150 | ppl     1.00\n",
      "| epoch  43 |   212/  269 batches | lr 0.002097 | 20.87 ms | loss 0.00158 | ppl     1.00\n",
      "| epoch  43 |   265/  269 batches | lr 0.002097 | 20.84 ms | loss 0.00123 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  43 | time:  5.96s | valid loss 0.00212 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  44 |    53/  269 batches | lr 0.002055 | 21.09 ms | loss 0.00088 | ppl     1.00\n",
      "| epoch  44 |   106/  269 batches | lr 0.002055 | 21.61 ms | loss 0.00097 | ppl     1.00\n",
      "| epoch  44 |   159/  269 batches | lr 0.002055 | 21.05 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  44 |   212/  269 batches | lr 0.002055 | 21.03 ms | loss 0.00073 | ppl     1.00\n",
      "| epoch  44 |   265/  269 batches | lr 0.002055 | 21.40 ms | loss 0.00088 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  44 | time:  6.02s | valid loss 0.00126 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  45 |    53/  269 batches | lr 0.002014 | 21.20 ms | loss 0.00080 | ppl     1.00\n",
      "| epoch  45 |   106/  269 batches | lr 0.002014 | 21.28 ms | loss 0.00077 | ppl     1.00\n",
      "| epoch  45 |   159/  269 batches | lr 0.002014 | 20.99 ms | loss 0.00095 | ppl     1.00\n",
      "| epoch  45 |   212/  269 batches | lr 0.002014 | 20.25 ms | loss 0.00096 | ppl     1.00\n",
      "| epoch  45 |   265/  269 batches | lr 0.002014 | 20.56 ms | loss 0.00106 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  45 | time: 13.45s | valid loss 0.00815 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  46 |    53/  269 batches | lr 0.001974 | 20.03 ms | loss 0.00115 | ppl     1.00\n",
      "| epoch  46 |   106/  269 batches | lr 0.001974 | 20.32 ms | loss 0.00103 | ppl     1.00\n",
      "| epoch  46 |   159/  269 batches | lr 0.001974 | 20.48 ms | loss 0.00118 | ppl     1.00\n",
      "| epoch  46 |   212/  269 batches | lr 0.001974 | 20.63 ms | loss 0.00123 | ppl     1.00\n",
      "| epoch  46 |   265/  269 batches | lr 0.001974 | 20.52 ms | loss 0.00095 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  46 | time:  5.80s | valid loss 0.00123 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  47 |    53/  269 batches | lr 0.001935 | 20.88 ms | loss 0.00083 | ppl     1.00\n",
      "| epoch  47 |   106/  269 batches | lr 0.001935 | 20.58 ms | loss 0.00072 | ppl     1.00\n",
      "| epoch  47 |   159/  269 batches | lr 0.001935 | 19.69 ms | loss 0.00087 | ppl     1.00\n",
      "| epoch  47 |   212/  269 batches | lr 0.001935 | 19.30 ms | loss 0.00125 | ppl     1.00\n",
      "| epoch  47 |   265/  269 batches | lr 0.001935 | 19.27 ms | loss 0.00088 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  47 | time:  5.67s | valid loss 0.00122 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  48 |    53/  269 batches | lr 0.001896 | 21.21 ms | loss 0.00077 | ppl     1.00\n",
      "| epoch  48 |   106/  269 batches | lr 0.001896 | 21.54 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  48 |   159/  269 batches | lr 0.001896 | 21.05 ms | loss 0.00079 | ppl     1.00\n",
      "| epoch  48 |   212/  269 batches | lr 0.001896 | 21.23 ms | loss 0.00091 | ppl     1.00\n",
      "| epoch  48 |   265/  269 batches | lr 0.001896 | 21.12 ms | loss 0.00105 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  48 | time:  6.02s | valid loss 0.00140 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  49 |    53/  269 batches | lr 0.001858 | 21.10 ms | loss 0.00083 | ppl     1.00\n",
      "| epoch  49 |   106/  269 batches | lr 0.001858 | 21.48 ms | loss 0.00081 | ppl     1.00\n",
      "| epoch  49 |   159/  269 batches | lr 0.001858 | 20.73 ms | loss 0.00126 | ppl     1.00\n",
      "| epoch  49 |   212/  269 batches | lr 0.001858 | 21.23 ms | loss 0.00100 | ppl     1.00\n",
      "| epoch  49 |   265/  269 batches | lr 0.001858 | 20.25 ms | loss 0.00082 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  49 | time:  5.93s | valid loss 0.00098 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  50 |    53/  269 batches | lr 0.001821 | 21.55 ms | loss 0.00083 | ppl     1.00\n",
      "| epoch  50 |   106/  269 batches | lr 0.001821 | 20.22 ms | loss 0.00084 | ppl     1.00\n",
      "| epoch  50 |   159/  269 batches | lr 0.001821 | 20.27 ms | loss 0.00112 | ppl     1.00\n",
      "| epoch  50 |   212/  269 batches | lr 0.001821 | 19.79 ms | loss 0.00118 | ppl     1.00\n",
      "| epoch  50 |   265/  269 batches | lr 0.001821 | 20.05 ms | loss 0.00089 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  50 | time: 13.62s | valid loss 0.00744 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  51 |    53/  269 batches | lr 0.001784 | 21.51 ms | loss 0.00088 | ppl     1.00\n",
      "| epoch  51 |   106/  269 batches | lr 0.001784 | 20.90 ms | loss 0.00074 | ppl     1.00\n",
      "| epoch  51 |   159/  269 batches | lr 0.001784 | 20.95 ms | loss 0.00082 | ppl     1.00\n",
      "| epoch  51 |   212/  269 batches | lr 0.001784 | 20.79 ms | loss 0.00072 | ppl     1.00\n",
      "| epoch  51 |   265/  269 batches | lr 0.001784 | 20.63 ms | loss 0.00094 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  51 | time:  5.95s | valid loss 0.00137 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  52 |    53/  269 batches | lr 0.001749 | 21.56 ms | loss 0.00093 | ppl     1.00\n",
      "| epoch  52 |   106/  269 batches | lr 0.001749 | 20.50 ms | loss 0.00083 | ppl     1.00\n",
      "| epoch  52 |   159/  269 batches | lr 0.001749 | 20.21 ms | loss 0.00107 | ppl     1.00\n",
      "| epoch  52 |   212/  269 batches | lr 0.001749 | 19.70 ms | loss 0.00122 | ppl     1.00\n",
      "| epoch  52 |   265/  269 batches | lr 0.001749 | 20.63 ms | loss 0.00091 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  52 | time:  5.85s | valid loss 0.00139 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  53 |    53/  269 batches | lr 0.001714 | 20.66 ms | loss 0.00076 | ppl     1.00\n",
      "| epoch  53 |   106/  269 batches | lr 0.001714 | 20.74 ms | loss 0.00071 | ppl     1.00\n",
      "| epoch  53 |   159/  269 batches | lr 0.001714 | 20.24 ms | loss 0.00075 | ppl     1.00\n",
      "| epoch  53 |   212/  269 batches | lr 0.001714 | 20.13 ms | loss 0.00072 | ppl     1.00\n",
      "| epoch  53 |   265/  269 batches | lr 0.001714 | 21.10 ms | loss 0.00079 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  53 | time:  5.85s | valid loss 0.00105 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  54 |    53/  269 batches | lr 0.001679 | 20.56 ms | loss 0.00078 | ppl     1.00\n",
      "| epoch  54 |   106/  269 batches | lr 0.001679 | 20.64 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  54 |   159/  269 batches | lr 0.001679 | 20.75 ms | loss 0.00081 | ppl     1.00\n",
      "| epoch  54 |   212/  269 batches | lr 0.001679 | 21.36 ms | loss 0.00095 | ppl     1.00\n",
      "| epoch  54 |   265/  269 batches | lr 0.001679 | 21.45 ms | loss 0.00091 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  54 | time:  5.94s | valid loss 0.00101 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  55 |    53/  269 batches | lr 0.001646 | 21.30 ms | loss 0.00076 | ppl     1.00\n",
      "| epoch  55 |   106/  269 batches | lr 0.001646 | 20.73 ms | loss 0.00067 | ppl     1.00\n",
      "| epoch  55 |   159/  269 batches | lr 0.001646 | 20.59 ms | loss 0.00079 | ppl     1.00\n",
      "| epoch  55 |   212/  269 batches | lr 0.001646 | 20.57 ms | loss 0.00081 | ppl     1.00\n",
      "| epoch  55 |   265/  269 batches | lr 0.001646 | 20.67 ms | loss 0.00078 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  55 | time: 13.51s | valid loss 0.00720 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  56 |    53/  269 batches | lr 0.001613 | 20.71 ms | loss 0.00075 | ppl     1.00\n",
      "| epoch  56 |   106/  269 batches | lr 0.001613 | 20.45 ms | loss 0.00085 | ppl     1.00\n",
      "| epoch  56 |   159/  269 batches | lr 0.001613 | 20.56 ms | loss 0.00120 | ppl     1.00\n",
      "| epoch  56 |   212/  269 batches | lr 0.001613 | 20.05 ms | loss 0.00121 | ppl     1.00\n",
      "| epoch  56 |   265/  269 batches | lr 0.001613 | 19.63 ms | loss 0.00089 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  56 | time:  5.76s | valid loss 0.00153 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  57 |    53/  269 batches | lr 0.001581 | 21.72 ms | loss 0.00081 | ppl     1.00\n",
      "| epoch  57 |   106/  269 batches | lr 0.001581 | 20.44 ms | loss 0.00069 | ppl     1.00\n",
      "| epoch  57 |   159/  269 batches | lr 0.001581 | 20.67 ms | loss 0.00077 | ppl     1.00\n",
      "| epoch  57 |   212/  269 batches | lr 0.001581 | 20.36 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  57 |   265/  269 batches | lr 0.001581 | 20.45 ms | loss 0.00076 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  57 | time:  5.88s | valid loss 0.00086 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  58 |    53/  269 batches | lr 0.001549 | 20.37 ms | loss 0.00074 | ppl     1.00\n",
      "| epoch  58 |   106/  269 batches | lr 0.001549 | 20.48 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  58 |   159/  269 batches | lr 0.001549 | 19.68 ms | loss 0.00072 | ppl     1.00\n",
      "| epoch  58 |   212/  269 batches | lr 0.001549 | 19.64 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  58 |   265/  269 batches | lr 0.001549 | 20.34 ms | loss 0.00079 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  58 | time:  5.72s | valid loss 0.00079 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  59 |    53/  269 batches | lr 0.001518 | 21.02 ms | loss 0.00081 | ppl     1.00\n",
      "| epoch  59 |   106/  269 batches | lr 0.001518 | 20.21 ms | loss 0.00127 | ppl     1.00\n",
      "| epoch  59 |   159/  269 batches | lr 0.001518 | 19.48 ms | loss 0.00115 | ppl     1.00\n",
      "| epoch  59 |   212/  269 batches | lr 0.001518 | 19.78 ms | loss 0.00086 | ppl     1.00\n",
      "| epoch  59 |   265/  269 batches | lr 0.001518 | 20.70 ms | loss 0.00076 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  59 | time:  5.75s | valid loss 0.00120 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  60 |    53/  269 batches | lr 0.001488 | 21.03 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  60 |   106/  269 batches | lr 0.001488 | 20.34 ms | loss 0.00064 | ppl     1.00\n",
      "| epoch  60 |   159/  269 batches | lr 0.001488 | 20.92 ms | loss 0.00080 | ppl     1.00\n",
      "| epoch  60 |   212/  269 batches | lr 0.001488 | 20.33 ms | loss 0.00087 | ppl     1.00\n",
      "| epoch  60 |   265/  269 batches | lr 0.001488 | 20.44 ms | loss 0.00075 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  60 | time: 13.36s | valid loss 0.00708 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  61 |    53/  269 batches | lr 0.001458 | 20.06 ms | loss 0.00072 | ppl     1.00\n",
      "| epoch  61 |   106/  269 batches | lr 0.001458 | 19.61 ms | loss 0.00069 | ppl     1.00\n",
      "| epoch  61 |   159/  269 batches | lr 0.001458 | 19.56 ms | loss 0.00068 | ppl     1.00\n",
      "| epoch  61 |   212/  269 batches | lr 0.001458 | 19.53 ms | loss 0.00064 | ppl     1.00\n",
      "| epoch  61 |   265/  269 batches | lr 0.001458 | 19.60 ms | loss 0.00074 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  61 | time:  5.60s | valid loss 0.00071 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  62 |    53/  269 batches | lr 0.001429 | 20.13 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  62 |   106/  269 batches | lr 0.001429 | 20.38 ms | loss 0.00076 | ppl     1.00\n",
      "| epoch  62 |   159/  269 batches | lr 0.001429 | 19.42 ms | loss 0.00089 | ppl     1.00\n",
      "| epoch  62 |   212/  269 batches | lr 0.001429 | 19.63 ms | loss 0.00115 | ppl     1.00\n",
      "| epoch  62 |   265/  269 batches | lr 0.001429 | 19.88 ms | loss 0.00084 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  62 | time:  5.65s | valid loss 0.00149 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  63 |    53/  269 batches | lr 0.001400 | 20.96 ms | loss 0.00072 | ppl     1.00\n",
      "| epoch  63 |   106/  269 batches | lr 0.001400 | 20.40 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  63 |   159/  269 batches | lr 0.001400 | 20.02 ms | loss 0.00071 | ppl     1.00\n",
      "| epoch  63 |   212/  269 batches | lr 0.001400 | 20.60 ms | loss 0.00072 | ppl     1.00\n",
      "| epoch  63 |   265/  269 batches | lr 0.001400 | 20.66 ms | loss 0.00070 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  63 | time:  5.83s | valid loss 0.00083 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  64 |    53/  269 batches | lr 0.001372 | 21.31 ms | loss 0.00074 | ppl     1.00\n",
      "| epoch  64 |   106/  269 batches | lr 0.001372 | 20.24 ms | loss 0.00071 | ppl     1.00\n",
      "| epoch  64 |   159/  269 batches | lr 0.001372 | 19.81 ms | loss 0.00078 | ppl     1.00\n",
      "| epoch  64 |   212/  269 batches | lr 0.001372 | 20.91 ms | loss 0.00087 | ppl     1.00\n",
      "| epoch  64 |   265/  269 batches | lr 0.001372 | 20.73 ms | loss 0.00086 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  64 | time:  5.86s | valid loss 0.00101 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  65 |    53/  269 batches | lr 0.001345 | 20.65 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  65 |   106/  269 batches | lr 0.001345 | 20.29 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  65 |   159/  269 batches | lr 0.001345 | 21.35 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  65 |   212/  269 batches | lr 0.001345 | 23.81 ms | loss 0.00074 | ppl     1.00\n",
      "| epoch  65 |   265/  269 batches | lr 0.001345 | 21.10 ms | loss 0.00068 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  65 | time: 13.62s | valid loss 0.00662 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  66 |    53/  269 batches | lr 0.001318 | 20.29 ms | loss 0.00069 | ppl     1.00\n",
      "| epoch  66 |   106/  269 batches | lr 0.001318 | 20.64 ms | loss 0.00061 | ppl     1.00\n",
      "| epoch  66 |   159/  269 batches | lr 0.001318 | 21.55 ms | loss 0.00068 | ppl     1.00\n",
      "| epoch  66 |   212/  269 batches | lr 0.001318 | 21.36 ms | loss 0.00071 | ppl     1.00\n",
      "| epoch  66 |   265/  269 batches | lr 0.001318 | 20.59 ms | loss 0.00086 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  66 | time:  5.93s | valid loss 0.00060 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  67 |    53/  269 batches | lr 0.001292 | 21.38 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  67 |   106/  269 batches | lr 0.001292 | 21.28 ms | loss 0.00062 | ppl     1.00\n",
      "| epoch  67 |   159/  269 batches | lr 0.001292 | 20.57 ms | loss 0.00068 | ppl     1.00\n",
      "| epoch  67 |   212/  269 batches | lr 0.001292 | 20.46 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  67 |   265/  269 batches | lr 0.001292 | 20.91 ms | loss 0.00074 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  67 | time:  5.95s | valid loss 0.00085 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  68 |    53/  269 batches | lr 0.001266 | 20.49 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  68 |   106/  269 batches | lr 0.001266 | 21.66 ms | loss 0.00068 | ppl     1.00\n",
      "| epoch  68 |   159/  269 batches | lr 0.001266 | 20.37 ms | loss 0.00075 | ppl     1.00\n",
      "| epoch  68 |   212/  269 batches | lr 0.001266 | 19.50 ms | loss 0.00067 | ppl     1.00\n",
      "| epoch  68 |   265/  269 batches | lr 0.001266 | 19.42 ms | loss 0.00078 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  68 | time:  5.75s | valid loss 0.00086 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  69 |    53/  269 batches | lr 0.001240 | 20.56 ms | loss 0.00077 | ppl     1.00\n",
      "| epoch  69 |   106/  269 batches | lr 0.001240 | 20.00 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  69 |   159/  269 batches | lr 0.001240 | 19.59 ms | loss 0.00074 | ppl     1.00\n",
      "| epoch  69 |   212/  269 batches | lr 0.001240 | 19.52 ms | loss 0.00083 | ppl     1.00\n",
      "| epoch  69 |   265/  269 batches | lr 0.001240 | 19.46 ms | loss 0.00076 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  69 | time:  5.63s | valid loss 0.00079 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  70 |    53/  269 batches | lr 0.001216 | 19.99 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  70 |   106/  269 batches | lr 0.001216 | 20.54 ms | loss 0.00072 | ppl     1.00\n",
      "| epoch  70 |   159/  269 batches | lr 0.001216 | 19.45 ms | loss 0.00076 | ppl     1.00\n",
      "| epoch  70 |   212/  269 batches | lr 0.001216 | 19.51 ms | loss 0.00062 | ppl     1.00\n",
      "| epoch  70 |   265/  269 batches | lr 0.001216 | 19.46 ms | loss 0.00084 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  70 | time: 13.46s | valid loss 0.00685 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  71 |    53/  269 batches | lr 0.001191 | 21.09 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  71 |   106/  269 batches | lr 0.001191 | 20.61 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  71 |   159/  269 batches | lr 0.001191 | 20.77 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  71 |   212/  269 batches | lr 0.001191 | 20.56 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  71 |   265/  269 batches | lr 0.001191 | 20.47 ms | loss 0.00075 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  71 | time:  5.88s | valid loss 0.00094 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  72 |    53/  269 batches | lr 0.001167 | 22.33 ms | loss 0.00067 | ppl     1.00\n",
      "| epoch  72 |   106/  269 batches | lr 0.001167 | 21.14 ms | loss 0.00062 | ppl     1.00\n",
      "| epoch  72 |   159/  269 batches | lr 0.001167 | 20.56 ms | loss 0.00071 | ppl     1.00\n",
      "| epoch  72 |   212/  269 batches | lr 0.001167 | 20.72 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  72 |   265/  269 batches | lr 0.001167 | 20.65 ms | loss 0.00075 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  72 | time:  5.98s | valid loss 0.00083 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  73 |    53/  269 batches | lr 0.001144 | 21.29 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  73 |   106/  269 batches | lr 0.001144 | 21.19 ms | loss 0.00062 | ppl     1.00\n",
      "| epoch  73 |   159/  269 batches | lr 0.001144 | 20.48 ms | loss 0.00071 | ppl     1.00\n",
      "| epoch  73 |   212/  269 batches | lr 0.001144 | 20.56 ms | loss 0.00075 | ppl     1.00\n",
      "| epoch  73 |   265/  269 batches | lr 0.001144 | 20.58 ms | loss 0.00070 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  73 | time:  5.91s | valid loss 0.00084 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  74 |    53/  269 batches | lr 0.001121 | 21.77 ms | loss 0.00064 | ppl     1.00\n",
      "| epoch  74 |   106/  269 batches | lr 0.001121 | 20.63 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  74 |   159/  269 batches | lr 0.001121 | 20.44 ms | loss 0.00067 | ppl     1.00\n",
      "| epoch  74 |   212/  269 batches | lr 0.001121 | 20.65 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  74 |   265/  269 batches | lr 0.001121 | 21.47 ms | loss 0.00074 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  74 | time:  5.96s | valid loss 0.00051 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  75 |    53/  269 batches | lr 0.001099 | 21.88 ms | loss 0.00063 | ppl     1.00\n",
      "| epoch  75 |   106/  269 batches | lr 0.001099 | 21.15 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  75 |   159/  269 batches | lr 0.001099 | 21.05 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  75 |   212/  269 batches | lr 0.001099 | 20.84 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  75 |   265/  269 batches | lr 0.001099 | 21.27 ms | loss 0.00087 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  75 | time: 14.34s | valid loss 0.00649 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  76 |    53/  269 batches | lr 0.001077 | 21.88 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  76 |   106/  269 batches | lr 0.001077 | 22.00 ms | loss 0.00059 | ppl     1.00\n",
      "| epoch  76 |   159/  269 batches | lr 0.001077 | 21.37 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  76 |   212/  269 batches | lr 0.001077 | 21.85 ms | loss 0.00082 | ppl     1.00\n",
      "| epoch  76 |   265/  269 batches | lr 0.001077 | 20.84 ms | loss 0.00069 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  76 | time:  6.98s | valid loss 0.00090 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  77 |    53/  269 batches | lr 0.001055 | 23.65 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  77 |   106/  269 batches | lr 0.001055 | 20.96 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  77 |   159/  269 batches | lr 0.001055 | 21.06 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  77 |   212/  269 batches | lr 0.001055 | 20.97 ms | loss 0.00073 | ppl     1.00\n",
      "| epoch  77 |   265/  269 batches | lr 0.001055 | 21.20 ms | loss 0.00074 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  77 | time:  6.12s | valid loss 0.00108 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  78 |    53/  269 batches | lr 0.001034 | 22.20 ms | loss 0.00067 | ppl     1.00\n",
      "| epoch  78 |   106/  269 batches | lr 0.001034 | 21.13 ms | loss 0.00061 | ppl     1.00\n",
      "| epoch  78 |   159/  269 batches | lr 0.001034 | 20.55 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  78 |   212/  269 batches | lr 0.001034 | 20.70 ms | loss 0.00061 | ppl     1.00\n",
      "| epoch  78 |   265/  269 batches | lr 0.001034 | 20.68 ms | loss 0.00081 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  78 | time:  5.97s | valid loss 0.00088 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  79 |    53/  269 batches | lr 0.001014 | 21.82 ms | loss 0.00068 | ppl     1.00\n",
      "| epoch  79 |   106/  269 batches | lr 0.001014 | 21.70 ms | loss 0.00058 | ppl     1.00\n",
      "| epoch  79 |   159/  269 batches | lr 0.001014 | 21.12 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  79 |   212/  269 batches | lr 0.001014 | 20.70 ms | loss 0.00072 | ppl     1.00\n",
      "| epoch  79 |   265/  269 batches | lr 0.001014 | 21.03 ms | loss 0.00074 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  79 | time:  6.02s | valid loss 0.00093 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  80 |    53/  269 batches | lr 0.000993 | 21.84 ms | loss 0.00064 | ppl     1.00\n",
      "| epoch  80 |   106/  269 batches | lr 0.000993 | 21.40 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  80 |   159/  269 batches | lr 0.000993 | 21.06 ms | loss 0.00063 | ppl     1.00\n",
      "| epoch  80 |   212/  269 batches | lr 0.000993 | 20.94 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  80 |   265/  269 batches | lr 0.000993 | 20.68 ms | loss 0.00065 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  80 | time: 13.98s | valid loss 0.00631 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  81 |    53/  269 batches | lr 0.000973 | 20.94 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  81 |   106/  269 batches | lr 0.000973 | 20.81 ms | loss 0.00062 | ppl     1.00\n",
      "| epoch  81 |   159/  269 batches | lr 0.000973 | 20.88 ms | loss 0.00067 | ppl     1.00\n",
      "| epoch  81 |   212/  269 batches | lr 0.000973 | 20.53 ms | loss 0.00070 | ppl     1.00\n",
      "| epoch  81 |   265/  269 batches | lr 0.000973 | 20.71 ms | loss 0.00073 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  81 | time:  5.90s | valid loss 0.00083 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  82 |    53/  269 batches | lr 0.000954 | 21.94 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  82 |   106/  269 batches | lr 0.000954 | 20.72 ms | loss 0.00058 | ppl     1.00\n",
      "| epoch  82 |   159/  269 batches | lr 0.000954 | 20.59 ms | loss 0.00068 | ppl     1.00\n",
      "| epoch  82 |   212/  269 batches | lr 0.000954 | 20.68 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  82 |   265/  269 batches | lr 0.000954 | 20.76 ms | loss 0.00068 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  82 | time:  5.94s | valid loss 0.00080 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  83 |    53/  269 batches | lr 0.000935 | 21.29 ms | loss 0.00064 | ppl     1.00\n",
      "| epoch  83 |   106/  269 batches | lr 0.000935 | 21.28 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  83 |   159/  269 batches | lr 0.000935 | 20.96 ms | loss 0.00058 | ppl     1.00\n",
      "| epoch  83 |   212/  269 batches | lr 0.000935 | 20.66 ms | loss 0.00059 | ppl     1.00\n",
      "| epoch  83 |   265/  269 batches | lr 0.000935 | 20.78 ms | loss 0.00062 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  83 | time:  5.95s | valid loss 0.00083 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  84 |    53/  269 batches | lr 0.000916 | 21.61 ms | loss 0.00061 | ppl     1.00\n",
      "| epoch  84 |   106/  269 batches | lr 0.000916 | 20.83 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  84 |   159/  269 batches | lr 0.000916 | 20.67 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  84 |   212/  269 batches | lr 0.000916 | 20.66 ms | loss 0.00059 | ppl     1.00\n",
      "| epoch  84 |   265/  269 batches | lr 0.000916 | 20.65 ms | loss 0.00068 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  84 | time:  5.93s | valid loss 0.00081 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  85 |    53/  269 batches | lr 0.000898 | 22.07 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  85 |   106/  269 batches | lr 0.000898 | 20.76 ms | loss 0.00059 | ppl     1.00\n",
      "| epoch  85 |   159/  269 batches | lr 0.000898 | 21.14 ms | loss 0.00065 | ppl     1.00\n",
      "| epoch  85 |   212/  269 batches | lr 0.000898 | 21.76 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  85 |   265/  269 batches | lr 0.000898 | 23.07 ms | loss 0.00062 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  85 | time: 14.52s | valid loss 0.00638 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  86 |    53/  269 batches | lr 0.000880 | 21.58 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  86 |   106/  269 batches | lr 0.000880 | 21.31 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  86 |   159/  269 batches | lr 0.000880 | 20.86 ms | loss 0.00061 | ppl     1.00\n",
      "| epoch  86 |   212/  269 batches | lr 0.000880 | 20.97 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  86 |   265/  269 batches | lr 0.000880 | 20.75 ms | loss 0.00069 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  86 | time:  5.99s | valid loss 0.00073 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  87 |    53/  269 batches | lr 0.000862 | 21.88 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  87 |   106/  269 batches | lr 0.000862 | 21.95 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  87 |   159/  269 batches | lr 0.000862 | 21.44 ms | loss 0.00059 | ppl     1.00\n",
      "| epoch  87 |   212/  269 batches | lr 0.000862 | 21.00 ms | loss 0.00058 | ppl     1.00\n",
      "| epoch  87 |   265/  269 batches | lr 0.000862 | 21.13 ms | loss 0.00066 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  87 | time:  6.09s | valid loss 0.00078 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  88 |    53/  269 batches | lr 0.000845 | 21.16 ms | loss 0.00057 | ppl     1.00\n",
      "| epoch  88 |   106/  269 batches | lr 0.000845 | 21.60 ms | loss 0.00057 | ppl     1.00\n",
      "| epoch  88 |   159/  269 batches | lr 0.000845 | 20.49 ms | loss 0.00062 | ppl     1.00\n",
      "| epoch  88 |   212/  269 batches | lr 0.000845 | 20.65 ms | loss 0.00066 | ppl     1.00\n",
      "| epoch  88 |   265/  269 batches | lr 0.000845 | 20.79 ms | loss 0.00065 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  88 | time:  5.94s | valid loss 0.00073 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  89 |    53/  269 batches | lr 0.000828 | 21.99 ms | loss 0.00062 | ppl     1.00\n",
      "| epoch  89 |   106/  269 batches | lr 0.000828 | 21.21 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  89 |   159/  269 batches | lr 0.000828 | 21.09 ms | loss 0.00062 | ppl     1.00\n",
      "| epoch  89 |   212/  269 batches | lr 0.000828 | 20.97 ms | loss 0.00059 | ppl     1.00\n",
      "| epoch  89 |   265/  269 batches | lr 0.000828 | 20.97 ms | loss 0.00062 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  89 | time:  6.03s | valid loss 0.00088 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  90 |    53/  269 batches | lr 0.000812 | 21.69 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  90 |   106/  269 batches | lr 0.000812 | 21.55 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  90 |   159/  269 batches | lr 0.000812 | 20.52 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  90 |   212/  269 batches | lr 0.000812 | 20.55 ms | loss 0.00055 | ppl     1.00\n",
      "| epoch  90 |   265/  269 batches | lr 0.000812 | 20.55 ms | loss 0.00061 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  90 | time: 13.95s | valid loss 0.00663 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  91 |    53/  269 batches | lr 0.000795 | 21.64 ms | loss 0.00064 | ppl     1.00\n",
      "| epoch  91 |   106/  269 batches | lr 0.000795 | 21.32 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  91 |   159/  269 batches | lr 0.000795 | 20.74 ms | loss 0.00062 | ppl     1.00\n",
      "| epoch  91 |   212/  269 batches | lr 0.000795 | 20.82 ms | loss 0.00057 | ppl     1.00\n",
      "| epoch  91 |   265/  269 batches | lr 0.000795 | 20.52 ms | loss 0.00063 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  91 | time:  5.96s | valid loss 0.00082 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  92 |    53/  269 batches | lr 0.000779 | 20.93 ms | loss 0.00060 | ppl     1.00\n",
      "| epoch  92 |   106/  269 batches | lr 0.000779 | 20.83 ms | loss 0.00054 | ppl     1.00\n",
      "| epoch  92 |   159/  269 batches | lr 0.000779 | 19.27 ms | loss 0.00058 | ppl     1.00\n",
      "| epoch  92 |   212/  269 batches | lr 0.000779 | 20.20 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  92 |   265/  269 batches | lr 0.000779 | 20.40 ms | loss 0.00064 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  92 | time:  5.78s | valid loss 0.00073 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  93 |    53/  269 batches | lr 0.000764 | 21.21 ms | loss 0.00061 | ppl     1.00\n",
      "| epoch  93 |   106/  269 batches | lr 0.000764 | 23.64 ms | loss 0.00057 | ppl     1.00\n",
      "| epoch  93 |   159/  269 batches | lr 0.000764 | 21.22 ms | loss 0.00063 | ppl     1.00\n",
      "| epoch  93 |   212/  269 batches | lr 0.000764 | 21.07 ms | loss 0.00068 | ppl     1.00\n",
      "| epoch  93 |   265/  269 batches | lr 0.000764 | 20.91 ms | loss 0.00070 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  93 | time:  6.13s | valid loss 0.00082 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  94 |    53/  269 batches | lr 0.000749 | 21.63 ms | loss 0.00064 | ppl     1.00\n",
      "| epoch  94 |   106/  269 batches | lr 0.000749 | 20.31 ms | loss 0.00057 | ppl     1.00\n",
      "| epoch  94 |   159/  269 batches | lr 0.000749 | 20.18 ms | loss 0.00058 | ppl     1.00\n",
      "| epoch  94 |   212/  269 batches | lr 0.000749 | 20.22 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  94 |   265/  269 batches | lr 0.000749 | 19.70 ms | loss 0.00060 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  94 | time:  5.80s | valid loss 0.00072 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  95 |    53/  269 batches | lr 0.000734 | 21.22 ms | loss 0.00057 | ppl     1.00\n",
      "| epoch  95 |   106/  269 batches | lr 0.000734 | 20.41 ms | loss 0.00055 | ppl     1.00\n",
      "| epoch  95 |   159/  269 batches | lr 0.000734 | 20.72 ms | loss 0.00058 | ppl     1.00\n",
      "| epoch  95 |   212/  269 batches | lr 0.000734 | 20.69 ms | loss 0.00059 | ppl     1.00\n",
      "| epoch  95 |   265/  269 batches | lr 0.000734 | 21.11 ms | loss 0.00065 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  95 | time: 14.26s | valid loss 0.00633 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  96 |    53/  269 batches | lr 0.000719 | 21.30 ms | loss 0.00057 | ppl     1.00\n",
      "| epoch  96 |   106/  269 batches | lr 0.000719 | 20.82 ms | loss 0.00054 | ppl     1.00\n",
      "| epoch  96 |   159/  269 batches | lr 0.000719 | 21.63 ms | loss 0.00059 | ppl     1.00\n",
      "| epoch  96 |   212/  269 batches | lr 0.000719 | 20.70 ms | loss 0.00055 | ppl     1.00\n",
      "| epoch  96 |   265/  269 batches | lr 0.000719 | 20.91 ms | loss 0.00060 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  96 | time:  5.99s | valid loss 0.00076 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  97 |    53/  269 batches | lr 0.000705 | 20.97 ms | loss 0.00055 | ppl     1.00\n",
      "| epoch  97 |   106/  269 batches | lr 0.000705 | 20.56 ms | loss 0.00052 | ppl     1.00\n",
      "| epoch  97 |   159/  269 batches | lr 0.000705 | 19.97 ms | loss 0.00059 | ppl     1.00\n",
      "| epoch  97 |   212/  269 batches | lr 0.000705 | 22.09 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  97 |   265/  269 batches | lr 0.000705 | 20.04 ms | loss 0.00058 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  97 | time:  5.95s | valid loss 0.00068 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  98 |    53/  269 batches | lr 0.000690 | 19.97 ms | loss 0.00056 | ppl     1.00\n",
      "| epoch  98 |   106/  269 batches | lr 0.000690 | 19.65 ms | loss 0.00054 | ppl     1.00\n",
      "| epoch  98 |   159/  269 batches | lr 0.000690 | 19.83 ms | loss 0.00059 | ppl     1.00\n",
      "| epoch  98 |   212/  269 batches | lr 0.000690 | 19.95 ms | loss 0.00054 | ppl     1.00\n",
      "| epoch  98 |   265/  269 batches | lr 0.000690 | 20.85 ms | loss 0.00059 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  98 | time:  5.71s | valid loss 0.00079 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  99 |    53/  269 batches | lr 0.000677 | 19.90 ms | loss 0.00053 | ppl     1.00\n",
      "| epoch  99 |   106/  269 batches | lr 0.000677 | 19.62 ms | loss 0.00053 | ppl     1.00\n",
      "| epoch  99 |   159/  269 batches | lr 0.000677 | 19.26 ms | loss 0.00058 | ppl     1.00\n",
      "| epoch  99 |   212/  269 batches | lr 0.000677 | 20.97 ms | loss 0.00058 | ppl     1.00\n",
      "| epoch  99 |   265/  269 batches | lr 0.000677 | 20.76 ms | loss 0.00059 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  99 | time:  5.72s | valid loss 0.00080 | valid ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch 100 |    53/  269 batches | lr 0.000663 | 20.31 ms | loss 0.00055 | ppl     1.00\n",
      "| epoch 100 |   106/  269 batches | lr 0.000663 | 20.83 ms | loss 0.00050 | ppl     1.00\n",
      "| epoch 100 |   159/  269 batches | lr 0.000663 | 19.82 ms | loss 0.00053 | ppl     1.00\n",
      "| epoch 100 |   212/  269 batches | lr 0.000663 | 20.49 ms | loss 0.00052 | ppl     1.00\n",
      "| epoch 100 |   265/  269 batches | lr 0.000663 | 19.37 ms | loss 0.00061 | ppl     1.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch 100 | time: 13.70s | valid loss 0.00625 | valid ppl     1.01\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(\n",
    "        train_data=train_data,\n",
    "        model = model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        epoch=epoch,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "    \n",
    "    if ( epoch % 5 == 0 ):\n",
    "        val_loss = plot_and_loss(\n",
    "            model, \n",
    "            val_data,\n",
    "            criterion,\n",
    "            epoch,\n",
    "            )\n",
    "        predict_future(\n",
    "            model, \n",
    "            val_data,\n",
    "            200)\n",
    "    else:\n",
    "        val_loss = evaluate(model, \n",
    "                            val_data,\n",
    "                            criterion\n",
    "                            )\n",
    "   \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    #if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    best_model = model\n",
    "\n",
    "    scheduler.step() \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
